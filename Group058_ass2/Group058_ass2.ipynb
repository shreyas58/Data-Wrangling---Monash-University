{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Date: 15/09/2019\n",
    "\n",
    "Version: 1.0\n",
    "\n",
    "Environment: Python 3 and Jupyter notebook\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Generating a sparse represtation for Paper Bodies\n",
    "\n",
    "### Importing libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pdfminer.six\n",
    "#installing pdfminer.six \n",
    "\n",
    "\n",
    "import pdfminer\n",
    "import io\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.layout import LAParams\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from urllib import request\n",
    "\n",
    "import requests\n",
    "\n",
    "import os \n",
    "\n",
    "import nltk\n",
    "\n",
    "from nltk import ngrams\n",
    "\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 1. Downloading PDF files from given URLs\n",
    "\n",
    "#### Note: An empty pdf folder was manually created, into which the pdf files were downloaded. An empty pdf folder must exist for the following code to work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Defining a function to convert pdf to text to perform processing on the text\n",
    "'''\n",
    "\n",
    "def convert_pdf_to_txt(path):\n",
    "    rsrcmgr = PDFResourceManager()\n",
    "    retstr = io.StringIO()\n",
    "    codec = 'utf-8'\n",
    "    laparams = LAParams()\n",
    "    device = TextConverter(rsrcmgr, retstr, codec=codec, laparams=laparams)\n",
    "    fp = open(path, 'rb')\n",
    "    interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
    "    password = \"\"\n",
    "    maxpages = 0\n",
    "    caching = True\n",
    "    pagenos = set()\n",
    "\n",
    "    for page in PDFPage.get_pages(fp, pagenos, maxpages=maxpages,\n",
    "                                  password=password,\n",
    "                                  caching=caching,\n",
    "                                  check_extractable=True):\n",
    "        interpreter.process_page(page)\n",
    "\n",
    "    text = retstr.getvalue()\n",
    "\n",
    "    fp.close()\n",
    "    device.close()\n",
    "    retstr.close()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "links =  convert_pdf_to_txt(\"./Group058.pdf\")\n",
    "#converting given pdf to a text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting the URLs from the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://drive.google.com/uc?export=download&id=1en9aeKBC5kxIJ5-40Hng1ONLl-s-3-Wz',\n",
       " 'https://drive.google.com/uc?export=download&id=1h1ZApxxSZ_0HlGiBmqnFQvbWysAti-UR',\n",
       " 'https://drive.google.com/uc?export=download&id=1u2TPAS9i-gRoQAYRVbIaDdY_2ktejzRL',\n",
       " 'https://drive.google.com/uc?export=download&id=1rdknW_Zh7pWWxk920Lq0sgxXPYVa9HdV',\n",
       " 'https://drive.google.com/uc?export=download&id=1VBdqTsR-1bWuHUUAGVHBxnselpxe4jlQ',\n",
       " 'https://drive.google.com/uc?export=download&id=1ie4Jz3CplWE3jSyXQ0tzXWq6n_BNfh4d',\n",
       " 'https://drive.google.com/uc?export=download&id=1hYyivtsETqrkYvI-Il6T9MvEHBtVdmXA',\n",
       " 'https://drive.google.com/uc?export=download&id=1Y6ISUUQY9eb7ClcqijYMVWEH6GLmyszx',\n",
       " 'https://drive.google.com/uc?export=download&id=1SGdLJfnrL2qZq0jaLzBo4Xu_msD5x-Cv',\n",
       " 'https://drive.google.com/uc?export=download&id=10jdrmb06Eo4eM0sH9zF5EtN7MCIOQxcZ',\n",
       " 'https://drive.google.com/uc?export=download&id=1tPOQS-G-lPREZpcAFyh2SC2-v8AE6EC6',\n",
       " 'https://drive.google.com/uc?export=download&id=1CZenT8GPErdPM_t6FleBJdmqLxhp9Zwk',\n",
       " 'https://drive.google.com/uc?export=download&id=10lk18OMXWdURfZzmi1sQxh9GghHHByH9',\n",
       " 'https://drive.google.com/uc?export=download&id=1YQx0mtoNt5jGkXozQdLaGkknZk-zv-tN',\n",
       " 'https://drive.google.com/uc?export=download&id=1VDIm2lbYvlrk-9_y41e_nv7Jo2_SFnUk',\n",
       " 'https://drive.google.com/uc?export=download&id=1-R-Ic6ZwHbMBHtoDNugoK3y7cPT7hVZ_',\n",
       " 'https://drive.google.com/uc?export=download&id=1Qh6htCvFKYcqLMjamdTOZ5ekCMiRIoCE',\n",
       " 'https://drive.google.com/uc?export=download&id=1HXQJxflU-80aCp_Onjq_OS4Nvyt9cbfD',\n",
       " 'https://drive.google.com/uc?export=download&id=1TQeZ4beXozorvFzVlNFoRkafzVE4QrDi',\n",
       " 'https://drive.google.com/uc?export=download&id=1fQ4B9Hiiar8Mv6ytyNgLOI99Z4RDWQqw',\n",
       " 'https://drive.google.com/uc?export=download&id=1Fw2DN9XUu9F71gjzWkAfrAfUsdKu0Caj',\n",
       " 'https://drive.google.com/uc?export=download&id=118q9VdsmtOwQgj0ns8OggRU_BV7E48K8',\n",
       " 'https://drive.google.com/uc?export=download&id=1TTz8kS7zwp21_8aZP4FaGaTzQe4lYHYV',\n",
       " 'https://drive.google.com/uc?export=download&id=1T-7TYn0k2vttMqzWsiYe684KkG_QpU2H',\n",
       " 'https://drive.google.com/uc?export=download&id=1MMMHL8_GvUvicmaXWJu8FipHtetrw6Ed',\n",
       " 'https://drive.google.com/uc?export=download&id=1gcPUXExeA8wsGs25v9r7Y4QKxWuqmMh7',\n",
       " 'https://drive.google.com/uc?export=download&id=1Wuh15rzj7cMfjjOBi1KQrgBngODGuQ4x',\n",
       " 'https://drive.google.com/uc?export=download&id=1CzjwumgSC6PgPIMiha8KIQEyvlTPCRcv',\n",
       " 'https://drive.google.com/uc?export=download&id=1F2eIirguzBaUiCbVoc1m4-T37cL2WZdq',\n",
       " 'https://drive.google.com/uc?export=download&id=1KMkBmPY7z_nyrfMuSz-UxFZ7VUdSQsfI',\n",
       " 'https://drive.google.com/uc?export=download&id=1yB0WtTJwncwbLwS-awYfiDfz5GMv0qhn',\n",
       " 'https://drive.google.com/uc?export=download&id=1sgHkM03n4lozmkDFdD_SNXgzbMeBCALM',\n",
       " 'https://drive.google.com/uc?export=download&id=1OcCoaCMUdVnHWH4paxgZ-Oj7hfnPFX1c',\n",
       " 'https://drive.google.com/uc?export=download&id=1lgJ34_5DObQ3w_ihDGZ3Mmbe7ySEcSxp',\n",
       " 'https://drive.google.com/uc?export=download&id=1jnkN7yY35G-XFHyhFeUFeLuyA7nyaHpD',\n",
       " 'https://drive.google.com/uc?export=download&id=1QpmXWYgAevIEnF3RwxWsQSVVqrdaGop4',\n",
       " 'https://drive.google.com/uc?export=download&id=1_fQJIeD16wZ9f9KFpWicHX-EJE7EAgs7',\n",
       " 'https://drive.google.com/uc?export=download&id=1BYSRELcaYYHVrsO8ofd4N8A2RwSkANXj',\n",
       " 'https://drive.google.com/uc?export=download&id=13lxhu-Ap0H0smX80rnNg_ulfGFxPyaGx',\n",
       " 'https://drive.google.com/uc?export=download&id=1idseZAHBFNH-0GOlJ9Al1YT18rmihFLv',\n",
       " 'https://drive.google.com/uc?export=download&id=1-Sb_0O9PCSB4KWuy3yjySV3KKNr5xs6P',\n",
       " 'https://drive.google.com/uc?export=download&id=1MOck5orGEqTuwiDi7iygYTh7PQrlPyMA',\n",
       " 'https://drive.google.com/uc?export=download&id=1NmfWLRWW7Vp2L0tiJ_vo1YBxsoul_s1n',\n",
       " 'https://drive.google.com/uc?export=download&id=1XzirabI8j-Yqijg7DYXI-2vwAgbmMJgq',\n",
       " 'https://drive.google.com/uc?export=download&id=1oBBpLA24BGybAMt7LMwmIr4zNvPfc9Qm',\n",
       " 'https://drive.google.com/uc?export=download&id=1c4aXR9gjf-K-ySZn631UZVKdwmQrzRmC',\n",
       " 'https://drive.google.com/uc?export=download&id=1i10JTvF30422mtY412e7H1pB6QkCYmSX',\n",
       " 'https://drive.google.com/uc?export=download&id=1W9LLuswhk9oqh4L6xGDjDgRMl7ZCfz4-',\n",
       " 'https://drive.google.com/uc?export=download&id=1-G7jTw1_l6Olq0PLvCLX7HuRJnY9V_rU',\n",
       " 'https://drive.google.com/uc?export=download&id=1lfm9hGXFMu7ZMjQaC6I4NX11uApBoBsY',\n",
       " 'https://drive.google.com/uc?export=download&id=1jj2TZgAmB6sULAenqGFgchtGQjP71IqX',\n",
       " 'https://drive.google.com/uc?export=download&id=1yX_NOvI_HINNgdy8n4_e9moypzzo8LBo',\n",
       " 'https://drive.google.com/uc?export=download&id=1HyemqVN6hPHytyyxlTwqI-eBwG3Aa4bc',\n",
       " 'https://drive.google.com/uc?export=download&id=1CBkQYEdHbxuRJJVIe-5fYhdSsApaW5tS',\n",
       " 'https://drive.google.com/uc?export=download&id=1aeJfJcozccTE-iDhWzYEw5TgiJ7fgWlu',\n",
       " 'https://drive.google.com/uc?export=download&id=1SOqP4f3939z5UburNo-3dfMg-BVF3M9K',\n",
       " 'https://drive.google.com/uc?export=download&id=1VEmTyu_5NAgJv7re7S-XVb1DUCXHQAId',\n",
       " 'https://drive.google.com/uc?export=download&id=1K822M5IbT1AlIxzyW01kwafqTIDJ2dsU',\n",
       " 'https://drive.google.com/uc?export=download&id=1itsTrj6o9XFaNoeLCwmaBh7FXDWtNIVd',\n",
       " 'https://drive.google.com/uc?export=download&id=1nWB4a1mjNFFSHrffl5ciMd46LsBPpRN4',\n",
       " 'https://drive.google.com/uc?export=download&id=1VresXJDM_xrT2wvr3ffM2_A930tG--Q9',\n",
       " 'https://drive.google.com/uc?export=download&id=1Db0Rf2puKPpR9JZ8NAolAsd8JyZ4xt3h',\n",
       " 'https://drive.google.com/uc?export=download&id=1ECU9LgK-0bh_8xjfyEsbmw-fhTjKOzt-',\n",
       " 'https://drive.google.com/uc?export=download&id=16jWhax3nI0dLerL6-yAwFcFoQG5506kN',\n",
       " 'https://drive.google.com/uc?export=download&id=1_u1I3eUe2lmmdpxPr07_2rm4vg5m13jH',\n",
       " 'https://drive.google.com/uc?export=download&id=1TI3fmSx2xI0mNYDti4M7N5BX4dzxg53z',\n",
       " 'https://drive.google.com/uc?export=download&id=1lbqUkuu07SZOGJEc9195weadcKdb4vLB',\n",
       " 'https://drive.google.com/uc?export=download&id=1eK2UqKiSUu8MZrW55sTWpCb3ayIQA7Jt',\n",
       " 'https://drive.google.com/uc?export=download&id=17QN6jyl9mr0BLFEyerZXa7ckC70mGJBs',\n",
       " 'https://drive.google.com/uc?export=download&id=10PVGtaVvmGXE5QbN02UmaZRKH9YWA7Fu',\n",
       " 'https://drive.google.com/uc?export=download&id=1G1H3uL65kclBib55Z7p42NhcCs9E1k_3',\n",
       " 'https://drive.google.com/uc?export=download&id=1KWPpavATG_mWrU8erBL6pAwnZUCUU6sT',\n",
       " 'https://drive.google.com/uc?export=download&id=11sgJGBYSHAUVEDWyKPn455eBGHKMIoa5',\n",
       " 'https://drive.google.com/uc?export=download&id=1sQM5uu-II3sya30xI6TtizZazcmhawly',\n",
       " 'https://drive.google.com/uc?export=download&id=1HTbhHRCQuLHEJc_0MqGJNAgy8qVELwyj',\n",
       " 'https://drive.google.com/uc?export=download&id=12pMuaox0aQD1J9bhvN9nsKzbo3DG__kD',\n",
       " 'https://drive.google.com/uc?export=download&id=1jffl0du8rI4MuVmJYCYGxTgxDXkClfkd',\n",
       " 'https://drive.google.com/uc?export=download&id=1dVJaY8WCDiOMX9gcleIYYNZ22pz1r9_l',\n",
       " 'https://drive.google.com/uc?export=download&id=1RMdZz2SxV6XfbMko4dznWzfYP2gpJLnH',\n",
       " 'https://drive.google.com/uc?export=download&id=1jfSqhiMA56KEeAnWbUhVHNVVfZ53JTm4',\n",
       " 'https://drive.google.com/uc?export=download&id=17Wq5pHwh0rnvwq84OcoS81R_aA66UBih',\n",
       " 'https://drive.google.com/uc?export=download&id=1ShlVsIy8eE-_OuGZVPpEdpqZw2w6eQPJ',\n",
       " 'https://drive.google.com/uc?export=download&id=1_vuyXVU4WpzMhpvYp7OHZaE7qq5xL_AN',\n",
       " 'https://drive.google.com/uc?export=download&id=1y99EjsQnBtM--o-JNeTRoPqyCKuMNmmQ',\n",
       " 'https://drive.google.com/uc?export=download&id=12zONC_lgj87JpyA0FnJIkKMa3ifq4zzA',\n",
       " 'https://drive.google.com/uc?export=download&id=1vl4a39yN4JPSR1K6YtuXaZGBUHz705lH',\n",
       " 'https://drive.google.com/uc?export=download&id=1RPBzXAliNNQzhooQcJWyprY_p3kut98X',\n",
       " 'https://drive.google.com/uc?export=download&id=1kDm4O30YAnn9fYbyCh74cmwSjOapVO_I',\n",
       " 'https://drive.google.com/uc?export=download&id=1t4Rh0-tSJnZd8BE3EgcuMAdR7Op34K1V',\n",
       " 'https://drive.google.com/uc?export=download&id=1EYF6xYNR4U5Sizus6ml_aR3-wCRmZVPU',\n",
       " 'https://drive.google.com/uc?export=download&id=13Y8oq1CFSJnuPT4n4iMFggiwG-Jang8q',\n",
       " 'https://drive.google.com/uc?export=download&id=1j80yrHbl37wXtXVDV9tt6sswqIGZ1vxR',\n",
       " 'https://drive.google.com/uc?export=download&id=1nNzwBzbTyr0djbmHHc2h7LBZr-s708s8',\n",
       " 'https://drive.google.com/uc?export=download&id=12biiXGfvt2uBMyPLksNT_y_TKKOghlMs',\n",
       " 'https://drive.google.com/uc?export=download&id=1x7yp1rjWRKtFPRmi_TG3OotpzqK9pNqy',\n",
       " 'https://drive.google.com/uc?export=download&id=1W9bglJ5AnGzBMcJhQ2ketz_UA6eX1c2n',\n",
       " 'https://drive.google.com/uc?export=download&id=1SJbBksFc3z_Htmo9Nmhb4AJK5-QEE7I7',\n",
       " 'https://drive.google.com/uc?export=download&id=1B26mulEkY6RF7K6wspdv3H3UbFvA9bg_',\n",
       " 'https://drive.google.com/uc?export=download&id=1bDQVRSC7fonuI55ZnWyZF_51XdW0wl8v',\n",
       " 'https://drive.google.com/uc?export=download&id=1VnT1jkw-o3hWMPNYrvjYoiHtKV1huqVo',\n",
       " 'https://drive.google.com/uc?export=download&id=1hzwIicU5pZdGtQ8JFX9nsFLA4acqEClL',\n",
       " 'https://drive.google.com/uc?export=download&id=1xzakTnJVvMpwnl2Qj00sNgzVnptdNcyI',\n",
       " 'https://drive.google.com/uc?export=download&id=1raCTfUektcuRIe2jj301rZ51MbblSSS_',\n",
       " 'https://drive.google.com/uc?export=download&id=14Mh0A2BgtXcYvLvB4PzZHJvJb-rjDeHu',\n",
       " 'https://drive.google.com/uc?export=download&id=1cZLOV9T88aOoP_LZutYlZNtQHnwadeDQ',\n",
       " 'https://drive.google.com/uc?export=download&id=1A2R3ZeXIYadrzp98jQMXFBjt3JF1AJWE',\n",
       " 'https://drive.google.com/uc?export=download&id=1CWaW9uAhS1041CbPIfM6p4xNBGLCFW1z',\n",
       " 'https://drive.google.com/uc?export=download&id=1Y0Lpx2-0hUjv8MzzSVEHUd_SRBkBnBkm',\n",
       " 'https://drive.google.com/uc?export=download&id=1fhy8JJ6jje7LIF4WL_E5msLHdMVcBvxG',\n",
       " 'https://drive.google.com/uc?export=download&id=1_cHi_oCk4-2e0TWuhU2bAIjg_sQKOByR',\n",
       " 'https://drive.google.com/uc?export=download&id=1EWOZfpzTZ_q0vnDRvmSvhlwOW-ekN4Et',\n",
       " 'https://drive.google.com/uc?export=download&id=1wCvavhoizFrQsPQ2DZh6J2tWzzfvz2-g',\n",
       " 'https://drive.google.com/uc?export=download&id=1JCjg2dIBL7RscNGKJH5bEYAV0ZhUcdAN',\n",
       " 'https://drive.google.com/uc?export=download&id=1SBqPCbjS5PPeHtb2DXseWkpOv-VP0Lpp',\n",
       " 'https://drive.google.com/uc?export=download&id=1wQK6_rGNTM134NlbOIyaS6exIWd9_tqa',\n",
       " 'https://drive.google.com/uc?export=download&id=1n-DR-wSKXPuGlzBBsHsNkK9VJn9AE35z',\n",
       " 'https://drive.google.com/uc?export=download&id=1HBFSLCTvgc-11nHuHZwbvVF6GANsLO3y',\n",
       " 'https://drive.google.com/uc?export=download&id=1ZjBaBPxggsA8Lrspe0eE1o0ftK81nXEk',\n",
       " 'https://drive.google.com/uc?export=download&id=170-3E98SlE-vinqXPsY-dG6_jIJ32D79',\n",
       " 'https://drive.google.com/uc?export=download&id=1qj0bUFwuxQkBsuczC0bjDDKHC8UgillU',\n",
       " 'https://drive.google.com/uc?export=download&id=1x7mWeqvJdk7Grhu52spKz5TrlqNSm7oZ',\n",
       " 'https://drive.google.com/uc?export=download&id=1Des_QWIO4u-7d7gATo36vL5IsOGzDF2-',\n",
       " 'https://drive.google.com/uc?export=download&id=16w7zSbe9nD8le6V6Jv-oxEPGpPVXJ2pQ',\n",
       " 'https://drive.google.com/uc?export=download&id=19Uem18r_6wiiGvHZzt3boW3kepED8ZEZ',\n",
       " 'https://drive.google.com/uc?export=download&id=11rQ95qtGS7nC0A1CYfXevAFYBVAOJzzo',\n",
       " 'https://drive.google.com/uc?export=download&id=10xL7ml1VpXz-vOiZhIAGgYyBzmOdzdlX',\n",
       " 'https://drive.google.com/uc?export=download&id=1mljVH1mycmgTg4zDQWeRSzM0kN-LBvRK',\n",
       " 'https://drive.google.com/uc?export=download&id=1fFNeeYciI1YYf11YMfiavg95VTgxsFcs',\n",
       " 'https://drive.google.com/uc?export=download&id=1MYc1infnVv0qyGsWwhypwFSztti4i5Yh',\n",
       " 'https://drive.google.com/uc?export=download&id=12HHyYCcFVE0J4PlWuPOu5CcCUOZOYPDA',\n",
       " 'https://drive.google.com/uc?export=download&id=1mD9NPSXGMCfnqzEHIerlQdVHN-fgGUC5',\n",
       " 'https://drive.google.com/uc?export=download&id=1DFY9BnBKleCYPEneMX-Tj9da-9sqnklk',\n",
       " 'https://drive.google.com/uc?export=download&id=1s_XAqggBGvgXvRAL6UBAShjP54kZPgh1',\n",
       " 'https://drive.google.com/uc?export=download&id=1tC7QaMv4PWUwm5JBvDPg1GRXSFtTIIaO',\n",
       " 'https://drive.google.com/uc?export=download&id=1ULToeCjxzEB4DaKqFgqWwBLdIFSal1r4',\n",
       " 'https://drive.google.com/uc?export=download&id=19NFYLF04gB80iXPNzwquIdyU8nkaMhmJ',\n",
       " 'https://drive.google.com/uc?export=download&id=1MB_q57WiZaWT60YYbUXfo5fm_O3_6C8g',\n",
       " 'https://drive.google.com/uc?export=download&id=1z-z7k8sGZC9qDzb4Mtk1eTGZmEbOvk8F',\n",
       " 'https://drive.google.com/uc?export=download&id=1Ot1rn37Ad5QNlH8idkEB4GEIn6HtOhYm',\n",
       " 'https://drive.google.com/uc?export=download&id=18FgkkduI9ohpEKTdUhdNRo5Iew7Rd9b5',\n",
       " 'https://drive.google.com/uc?export=download&id=1HgkjUAkxP2cgz9eazta0BzLmB8CogdkK',\n",
       " 'https://drive.google.com/uc?export=download&id=1tam1_TMCGiBv7sXJXSoy1MB4vUhidLQe',\n",
       " 'https://drive.google.com/uc?export=download&id=1cx0umG8w7Q-w-ElI0E4bB1Q9f34TjQqc',\n",
       " 'https://drive.google.com/uc?export=download&id=1bhofuAFT4TfWdfoW8tq6cBi4YVIVARxc',\n",
       " 'https://drive.google.com/uc?export=download&id=1p560f-VuH6WoF5UbaLXJnBIUrxGzeAql',\n",
       " 'https://drive.google.com/uc?export=download&id=1518xM_WTvF3s27BgLdCtXXwuRtSeWC-Y',\n",
       " 'https://drive.google.com/uc?export=download&id=1AZZEUX-ER4oSEhNBpJe2cJpQZltkMPC4',\n",
       " 'https://drive.google.com/uc?export=download&id=16Ic_7lmHoREaMXl3APNWF2K-kHNUwmFS',\n",
       " 'https://drive.google.com/uc?export=download&id=1z1Q714QllF4pcl04a2SFHHvQphpjEqeu',\n",
       " 'https://drive.google.com/uc?export=download&id=1-T8GcMg1fI1O7pfxC5QmymGv9acxFbID',\n",
       " 'https://drive.google.com/uc?export=download&id=11BxXkZU8ay3yJmA53Om34wkPnyfdMzCc',\n",
       " 'https://drive.google.com/uc?export=download&id=1UhDeiCHbfQUxRIUG7dON_dlmszYtP3w6',\n",
       " 'https://drive.google.com/uc?export=download&id=1AqvuQMk4YwVIueg6sv7U0A09Ss7bY_n5',\n",
       " 'https://drive.google.com/uc?export=download&id=11JTfMPFqU3Kvd32SUK0XzAtUn6kDL_aW',\n",
       " 'https://drive.google.com/uc?export=download&id=17tjeCBDVgJPFURjOlfwTTxN2lozbx0HM',\n",
       " 'https://drive.google.com/uc?export=download&id=1c5G-OdjSPhyyVAbKtCcofiSGChcMQqv4',\n",
       " 'https://drive.google.com/uc?export=download&id=1ZG86fz_dw_u7G8XQPtcBUvr7Vt5mnjZP',\n",
       " 'https://drive.google.com/uc?export=download&id=191CqjNw7j-AfCO2Z4AWhRI9GEzmBP5zg',\n",
       " 'https://drive.google.com/uc?export=download&id=1bwVH8BKtSmvmflEQhR8UBFf-WQRQAtLS',\n",
       " 'https://drive.google.com/uc?export=download&id=1szFdCX4JReoznRaK7McpnK23ovar5tpi',\n",
       " 'https://drive.google.com/uc?export=download&id=1ixB4faKGWdYVHUpfVvdpFFy-MN920wFE',\n",
       " 'https://drive.google.com/uc?export=download&id=1WMpPttlFWRjXdA_mWhWhdqKBIMqM7Lyk',\n",
       " 'https://drive.google.com/uc?export=download&id=11O4G9c5BOLcjRNgE-sLgGlAvPcRNMCCE',\n",
       " 'https://drive.google.com/uc?export=download&id=1Tjah3tqs4MYJxCtI5kLUdrMWdKttbXAP',\n",
       " 'https://drive.google.com/uc?export=download&id=1gNUxe5-wnFHCH8JWGpLCxPD3gp9v8d2B',\n",
       " 'https://drive.google.com/uc?export=download&id=1dXz46hizkqGTNbryXa3QQKeucjpSpuMx',\n",
       " 'https://drive.google.com/uc?export=download&id=1j5MQ-vtxtBAUEO5rYXBMCyaUWCDOkWKI',\n",
       " 'https://drive.google.com/uc?export=download&id=1Uhx--CyzphPV1mPXGltftuyktkJ1Ln8H',\n",
       " 'https://drive.google.com/uc?export=download&id=1ZRqPzMQ2fzUlOZmC5zsc5Cbd8l72E81Y',\n",
       " 'https://drive.google.com/uc?export=download&id=1OihxKRlJqGJG5a4GGRI0oz2TCL5ziCCl',\n",
       " 'https://drive.google.com/uc?export=download&id=18JkLZp4Ah6tlDwZqI8hqHh-STWCpXevM',\n",
       " 'https://drive.google.com/uc?export=download&id=11TB4jBzm5E9Cks1sodDSSmIUUsofOYbt',\n",
       " 'https://drive.google.com/uc?export=download&id=1gUlVp01zB65Se5aupcYx_330l2IdWhk2',\n",
       " 'https://drive.google.com/uc?export=download&id=18NJM6kBT0aHX2Jgb-5z4wJYB2v9zRsh_',\n",
       " 'https://drive.google.com/uc?export=download&id=1lCVsR27jwAsPg4HwWuNejujSfAUmCX0b',\n",
       " 'https://drive.google.com/uc?export=download&id=1e-agB135Qh-U3EsQciIyhRgg9VoUe8Cf',\n",
       " 'https://drive.google.com/uc?export=download&id=1ztIhL6ItmYPk_10HjgP7Pt7ZCNm-NtEp',\n",
       " 'https://drive.google.com/uc?export=download&id=1CLkU5-WDlWaFREe2fhka7Q6OTnTEvhMU',\n",
       " 'https://drive.google.com/uc?export=download&id=1KuPb9AB5l2dQRfqLrUMhsJ73UXG-cOLe',\n",
       " 'https://drive.google.com/uc?export=download&id=1BrmkVl3rJK6B10dUADqy8lPkf4fhfbhQ',\n",
       " 'https://drive.google.com/uc?export=download&id=10ZEyD17mBSMT93u1TBKzI8_BSLd1p8KE',\n",
       " 'https://drive.google.com/uc?export=download&id=1jvTDab7Pxcslzjw5bipTJpF5qsaEYVqW',\n",
       " 'https://drive.google.com/uc?export=download&id=1WefsP0m10dus-JmpfwslvzQos9HDqy2W',\n",
       " 'https://drive.google.com/uc?export=download&id=1aPMCu3Q9JsOxYxehB_nRfh-p3X5FjCAO',\n",
       " 'https://drive.google.com/uc?export=download&id=1geckD1j00kCmnygxJPzmh4vj9uLcOOsp',\n",
       " 'https://drive.google.com/uc?export=download&id=1AWE8mv9zfakxvxKOfeyDp3u5p-7IZzEy',\n",
       " 'https://drive.google.com/uc?export=download&id=1NqovZ9gCx_UsvLyfd-BxCQE8gO_40Z3n',\n",
       " 'https://drive.google.com/uc?export=download&id=1ThjfwGwIGOQeuK30LXjQ7bCa-qkfnaBY',\n",
       " 'https://drive.google.com/uc?export=download&id=17omRELQOyc1cgaFPfH2g8xydFlXqHGrH',\n",
       " 'https://drive.google.com/uc?export=download&id=1iwgakU_A50qz5a8-_1JOi0BNSVc3fpIl',\n",
       " 'https://drive.google.com/uc?export=download&id=1zDuzzj4La2-yolBA5G6GjJ0x_O1apqi7',\n",
       " 'https://drive.google.com/uc?export=download&id=17naDu4FD1la7UYbtboVMzFBWIfkfk9y7',\n",
       " 'https://drive.google.com/uc?export=download&id=1pCVlxMdzjB12kUKnwAQxsil4uGqbvktK',\n",
       " 'https://drive.google.com/uc?export=download&id=1pr3ySbxa_SVe8DtUHyXWqtlu21i7mKHF',\n",
       " 'https://drive.google.com/uc?export=download&id=12eUzlc1g2bwHvgwsp2anFldViXhjQ69g',\n",
       " 'https://drive.google.com/uc?export=download&id=1whPE0UVyR7llyBRBgSnXn-KBwkHYVAPA',\n",
       " 'https://drive.google.com/uc?export=download&id=1LklZndCJL3bpjEMgfBnKom4VinJ9fJTv',\n",
       " 'https://drive.google.com/uc?export=download&id=1Umy0O-Jgcke9zYiFE_59s62Sxtc-JcB_',\n",
       " 'https://drive.google.com/uc?export=download&id=1dTzufnEJGXasJLAwyahyHG2h7B0YrzSj',\n",
       " 'https://drive.google.com/uc?export=download&id=1GKkY4ntyUuqPmv8ifbYXBJrDzZODQ1Mp']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "links = re.sub(r'\\n', '    ', links)\n",
    "links = re.sub('PP[0-9]{4}.pdf', '', links)\n",
    "links = re.sub(\"filename\", '', links)\n",
    "links = re.sub(r\"url\", '', links)\n",
    "links = re.sub(r\"\\x0c\", '', links)\n",
    "#using python's regular expression to extract URLs from the text\n",
    "\n",
    "links = links.split()\n",
    "links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading files from each URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Downloading files by iterating through the Link list and assigning names to them using a counter named \"i\"\n",
    "\n",
    "i = 1\n",
    "for item in links:\n",
    "    i = str(i)\n",
    "    url = item\n",
    "    pdfName = str(r\"./PDF/\" + i + r\".pdf\")\n",
    "    theFile = requests.get(url)\n",
    "    open(pdfName, 'wb').write(theFile.content)\n",
    "    i = int(i)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Reading the PDF files into text and extracting the required entities\n",
    "\n",
    "### a. Extracting paper bodies from each of downloaded PDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracting the Paper bodies from each pdf and making a list out of 200 bodies\n",
    "\n",
    "bodies = [''] * 200\n",
    "#initialising list to store paper bodies frome each pdf\n",
    "\n",
    "for i in range(1,201):\n",
    "    i = str(i)\n",
    "    fileName = \"./PDF/\" + i + \".pdf\"\n",
    "    fileContents = convert_pdf_to_txt(fileName)\n",
    "    fileContents = re.sub('\\n', ' ', fileContents)\n",
    "    start = re.search('1 Paper Body', fileContents).start()\n",
    "    end = re.search('2 References', fileContents).end()\n",
    "    fileContents = fileContents[start:end-14]\n",
    "    fileContents = re.sub('1 Paper Body', '', fileContents)\n",
    "    i = int(i)\n",
    "    bodies[i-1] = fileContents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Sentence Segmentation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "#loading packages for sentence detection\n",
    "\n",
    "segmenting = bodies\n",
    "\n",
    "for i in range(len(bodies)):\n",
    "    segmenting[i] = sent_detector.tokenize(bodies[i].strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c.  first word of each Sentence to lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "lowerCasing = segmenting\n",
    "\n",
    "for i in range(len(segmenting)):\n",
    "    for j in range(len(segmenting[i])):\n",
    "        lowerCasing[i][j] = segmenting[i][j][0].lower() + segmenting[i][j][1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d. Word Tokenisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting sentences into tokens based on the given regular expression\n",
    "\n",
    "tokens = [''] * len(lowerCasing)\n",
    "\n",
    "for i in range(len(lowerCasing)):\n",
    "    tokens[i] = str(lowerCasing[i])\n",
    "    tokens[i] = re.findall(r\"[A-Za-z]\\w+(?:[-'?]\\w+)?\", tokens[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  e. Finding Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigramTokens = [[] for i in range(len(tokens))]\n",
    "\n",
    "token = [''] * len(tokens)\n",
    "\n",
    "for i in range(len(tokens)):\n",
    "    token[i] = str(tokens[i])\n",
    "    token[i] = re.sub(r\"\\[\", '', token[i])\n",
    "    token[i] = re.sub(r\"\\]\", '', token[i])\n",
    "    token[i] = re.sub(r\"\\'\", '', token[i])\n",
    "    token[i] = re.sub(r\"\\'\", '', token[i])\n",
    "    token[i] = re.sub(r\"\\,\", '', token[i])\n",
    "    \n",
    "    n = 2\n",
    "    twograms = ngrams(token[i].split(), n)\n",
    "    #using ngrams function to find bigrams\n",
    "    for grams in twograms:\n",
    "        bigramTokens[i].append(grams)\n",
    "\n",
    "        \n",
    "uniqueBigramTokens = [''] * len(bigramTokens)\n",
    "\n",
    "for i in range(len(bigramTokens)):\n",
    "    uniqueBigramTokens[i] = list(set(bigramTokens[i]))\n",
    " \n",
    "    #Converting unique tokens into x_y format for calculating IDFs\n",
    "\n",
    "for i in range(len(uniqueBigramTokens)):\n",
    "    for j in range(len(uniqueBigramTokens[i])):\n",
    "        uniqueBigramTokens[i][j] = uniqueBigramTokens[i][j][0] + \"_\" + uniqueBigramTokens[i][j][1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigramTk = bigramTokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a single list of Bigrams\n",
    "bigramCombined = []\n",
    "\n",
    "for i in range(len(bigramTokens)):\n",
    "    for item in bigramTokens[i]:\n",
    "        bigramCombined.append(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## f. Filtering Bigrams\n",
    "\n",
    "### Removing context independent stop words and word with length less than 3 from the Bigrams list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading the given context independent stop word list\n",
    "stopWords = open('stopwords_en.txt', 'r').read()\n",
    "stopWords = stopWords.split('\\n')\n",
    "\n",
    "bigramTokens_NoStopWords = []\n",
    "\n",
    "for i in range(len(bigramCombined)):\n",
    "    x,y = bigramCombined[i]\n",
    "    if x.lower() in stopWords or y.lower() in stopWords or len(x) < 3 or len(y) < 3:\n",
    "        pass\n",
    "    else:\n",
    "        bigramTokens_NoStopWords.append(x + \"_\" + y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing context dependent stop words and rare tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculating frequency of bigrams and taking top 4000 (we need only 200, taking top 4000 to reduce computation time)\n",
    "\n",
    "bigramFreqs = FreqDist(bigramTokens_NoStopWords).most_common(4000)\n",
    "#bigramFreqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculating IDF for the bigrams\n",
    "\n",
    "bigramidf = {}\n",
    "#initialising a dictionary to store IDF values for each bigram\n",
    "\n",
    "for item in bigramFreqs:\n",
    "    x,y = item\n",
    "    count = 0\n",
    "    for i in range(len(uniqueBigramTokens)):\n",
    "        if x in uniqueBigramTokens[i]:\n",
    "            count += 1\n",
    "        bigramidf[item] = count\n",
    "\n",
    "#bigramidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Filtering bigrams and removing bigrams with rare tokens (threshold = 3%) and context-independent tokens (threshold = 95%) \n",
    "bigramIDF = {}\n",
    "\n",
    "upperThreshold = 0.95 * len(tokens)\n",
    "lowerThreshold = .03 * len(tokens)\n",
    "\n",
    "for key in bigramidf:\n",
    "    if bigramidf[key] <= upperThreshold and bigramidf[key] > lowerThreshold:\n",
    "        bigramIDF[key] = bigramidf[key]\n",
    "        \n",
    "#bigramIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Taking only bigram and frequence and then sorting them\n",
    "\n",
    "topBigrams = []\n",
    "\n",
    "for key in bigramIDF:\n",
    "    x,y = key\n",
    "    tup = (x,y)\n",
    "    topBigrams.append(tup)\n",
    "\n",
    "topBigrams = sorted(topBigrams, key = lambda tup: (-tup[1], tup[0]))\n",
    "topBigrams = topBigrams[:200]\n",
    "#topBigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams = [] \n",
    "\n",
    "for i in range(len(topBigrams)):\n",
    "        bigrams.append(topBigrams[i][0])\n",
    "#bigrams    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## g. Finding Unigrams\n",
    "\n",
    "### Removing Stop Words and Words with a length of less than 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens2 = [[]for i in range(len(tokens))]\n",
    "tokens3 = [[]for i in range(len(tokens))]\n",
    "\n",
    "for i in range(len(tokens)):\n",
    "    for item in tokens[i]:\n",
    "        if item not in stopWords:\n",
    "            tokens2[i].append(item)\n",
    "\n",
    "for i in range(len(tokens2)):\n",
    "    for item in tokens2[i]:\n",
    "        if len(item) > 3:\n",
    "            tokens3[i].append(item)            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming tokens using PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "#loading stemmer function \n",
    "\n",
    "stemmedTokens = [[] for i in range(len(tokens))]\n",
    "#creating a list to store stemmed tokens\n",
    "\n",
    "for i in range(len(tokens3)):\n",
    "    for item in tokens3[i]:\n",
    "        if item is item.lower():\n",
    "            item = stemmer.stem(item)\n",
    "            stemmedTokens[i].append(item)\n",
    "        else:\n",
    "            stemmedTokens[i].append(item)\n",
    "#for loop to stem each word in tokens2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding all unigrams into a single list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmedTokensCombined = []\n",
    "#crearting a list to store all unigrams\n",
    "\n",
    "for i in range(len(stemmedTokens)):\n",
    "    for items in stemmedTokens[i]:\n",
    "        stemmedTokensCombined.append(items)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a set of tokens for each pdf file\n",
    "\n",
    "uniqueStemmedTokens = [''] * len(stemmedTokens) \n",
    "#creating an empty list to store unique tokens from each pdf\n",
    "\n",
    "for i in range(len(stemmedTokens)):\n",
    "    uniqueStemmedTokens[i] = list(set(stemmedTokens[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering out Rare tokens and context dependent stop words from Unigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculating frequency of unigrams and taking top 4000 (we need only 200, taking top 4000 to reduce computation time)\n",
    "\n",
    "unigramFreqs = FreqDist(stemmedTokensCombined).most_common()\n",
    "# unigramFreqs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating IDF for unigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigramidf = {}\n",
    "#initialising a dictionary to store IDF values for each unigram\n",
    "\n",
    "for x,y in unigramFreqs:\n",
    "    count = 0\n",
    "    for items in uniqueStemmedTokens:\n",
    "        if x in items:\n",
    "            count += 1\n",
    "    unigramidf[(x,y)] = count\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filtering unigrams and removing bigrams with rare tokens (threshold = 3%) and context-independent tokens (threshold = 95%) \n",
    "\n",
    "unigramIDF = {}\n",
    "#creating dictionary to store filtered unigrams\n",
    "\n",
    "upperThreshold = 0.95 * len(tokens)\n",
    "lowerThreshold = .03 * len(tokens)\n",
    "\n",
    "for key in unigramidf:\n",
    "    if unigramidf[key] > upperThreshold or unigramidf[key] < lowerThreshold:\n",
    "        pass\n",
    "    else:\n",
    "        unigramIDF[key] = unigramidf[key]\n",
    "\n",
    "# unigramIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigramIdf = []\n",
    "\n",
    "for key in unigramIDF:\n",
    "    unigramIdf.append(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigrams = []\n",
    "\n",
    "for i in range(len(unigramIdf)):\n",
    "    unigrams.append(unigramIdf[i][0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## h. Creating vocabulary (unigrams and bigrams combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = unigrams + bigrams\n",
    "#combining words in unigrams and bigrams and storing it in vocab\n",
    "\n",
    "vocab = sorted(vocab)\n",
    "#sorting words in vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabDict = {}\n",
    "#creating a dictionary to store words in vocan with a count\n",
    "\n",
    "i = 0\n",
    "for item in vocab:\n",
    "    vocabDict[item] = i\n",
    "    i+= 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a list with ID's assigned to the words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialising tokensID to store  token ids for independent bodies\n",
    "tokensID = [[] for i in range(len(tokens))]\n",
    "\n",
    "#Iterating through items in tokens and appending the same using their ID's\n",
    "for i in range(len(tokens)):\n",
    "    for item in tokens[i]:\n",
    "        if item in unigrams:\n",
    "            tokensID[i].append(vocabDict[item])\n",
    "# tokensID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Substituting words with ID for bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a list of bigram tokens with the IDs assigned to them\n",
    "bigTokens = bigramTk\n",
    "bigramTokens = [[] for i in range(len(bigTokens))]\n",
    "\n",
    "for i in range(len(bigTokens)):\n",
    "    for x,y in bigTokens[i]:\n",
    "        #Converting them to the format saved in the dictionary\n",
    "        item = x + \"_\" + y\n",
    "        bigramTokens[i].append(item)\n",
    "# bigramTokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigramID = [[] for i in range(len(bigramTokens))]\n",
    "\n",
    "#Storing them as IDs \n",
    "for i in range(len(bigramTokens)):\n",
    "    for item in bigramTokens[i]:\n",
    "        if item in bigrams:\n",
    "            bigramID[i].append(vocabDict[item])\n",
    "\n",
    "# bigramID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Unigram final counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Taking frequency count of the unigrams from tokensID\n",
    "unigramFinalCount = [[] for i in range(len(tokensID))]\n",
    "\n",
    "for i in range(len(tokensID)):\n",
    "    unigramFinalCount[i] = FreqDist(tokensID[i]).most_common()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Bigram final count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Taking frequency count of the bigrams from tokensID\n",
    "bigramFinalCount = [[] for i in range(len(bigramID))]\n",
    "\n",
    "for i in range(len(bigramID)):\n",
    "    bigramFinalCount[i] = FreqDist(bigramID[i]).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Retrieving the unique file codes to create the required vector\n",
    "fileID =  convert_pdf_to_txt(\"./Group058.pdf\")\n",
    "\n",
    "fileID = re.sub('https(.*?)\\\\n', '', fileID)\n",
    "fileID = re.sub('filename\\\\n\\\\nurl\\\\n\\\\n', '', fileID)\n",
    "fileID = re.sub('\\\\n', '', fileID)\n",
    "fileID = re.sub('\\\\x0c', '', fileID)\n",
    "fileID = re.sub('\\.pdf', '', fileID)\n",
    "fileID = fileID.split()\n",
    "\n",
    "# fileID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Storing the vector contents into a variable called vector (Adding contents from Unigram and Bigram)\n",
    "vector = ''\n",
    "\n",
    "for i in range(len(fileID)):\n",
    "    vector = vector + fileID[i] + \",\"\n",
    "    for x,y in unigramFinalCount[i]:\n",
    "        vector = vector + str(x) + \":\" + str(y) + \",\"\n",
    "    for x,y in bigramFinalCount[i]:\n",
    "        vector = vector + str(x) + \":\" + str(y) + \",\"\n",
    "    vector = vector + \"\\n\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Line before new file starts\n",
    "vector = re.sub(',\\n', '\\n', vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "826729"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Opening file and writing contents of variable vector into it\n",
    "vectorFile = open(\"./Group058_count_vectors.txt\",\"w+\")\n",
    "vectorFile.write(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocabDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Writing the vocabulary (The dictionary with words-[Unigrams and Bigrams] and unique ID)\n",
    "vocabStr = ''\n",
    "voc = open(\"./Group058_vocab.txt\", \"w+\", encoding= \"utf-8\")\n",
    "\n",
    "for key in vocabDict:\n",
    "    vocabssss = str(key) + \":\" + str(vocabDict[key])  \n",
    "    voc.write(vocabssss + '\\n')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: Generating CSV file containing top 10 most frequent words in titles, authors and abstracts\n",
    "\n",
    "## 1. Finding Titles\n",
    "\n",
    "### a. Extracting Titles from pdf files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracting the Titles from each pdf \n",
    "\n",
    "titles = [''] * 200\n",
    "#initialising list to store titles from each pdf file\n",
    "\n",
    "for i in range(1,201):\n",
    "    i = str(i)\n",
    "    fileName = \"./PDF/\" + i + \".pdf\"\n",
    "    #reading each pdf in the PDF folder\n",
    "    \n",
    "    fileContents = convert_pdf_to_txt(fileName)\n",
    "    #coverting pdf to file\n",
    "    \n",
    "    fileContents = re.sub('\\n', ' ', fileContents)\n",
    "    end = re.search('Authored by:', fileContents).end()\n",
    "    title = fileContents[0:end-14]\n",
    "    #finding start and end index of titles in each pdf and then extracting the title\n",
    "        \n",
    "    i = int(i)\n",
    "    titles[i-1] = title\n",
    "    #storing titles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b. Performing Tokenization on Titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_tokens = [''] * len(titles)\n",
    "#initialising list to store title tokens\n",
    "\n",
    "for i in range(len(titles)):\n",
    "    title_tokens[i] = str(titles[i])\n",
    "    title_tokens[i] = re.sub('-\\s','',title_tokens[i])\n",
    "    title_tokens[i] = re.findall(r\"[A-Za-z]\\w+(?:[-'?]\\w+)?\", title_tokens[i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c. Normalising title tokens to lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(title_tokens)):\n",
    "    for j in range(len(title_tokens[i])):\n",
    "        title_tokens[i][j] =  title_tokens[i][j].lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## d. Removing context-independent stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_tokens2 = [[]for i in range(len(title_tokens))]\n",
    "#initialising list to store filtered tokens\n",
    "\n",
    "for i in range(len(title_tokens)):\n",
    "    for words in title_tokens[i]:\n",
    "        if words not in stopWords:\n",
    "            title_tokens2[i].append(words)\n",
    "#filtering out stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## e. Creating a word count on Unique Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "tkn = []\n",
    "#initialising list to store unique tokens\n",
    "\n",
    "for i in range(len(title_tokens2)):\n",
    "    for word in title_tokens2[i]:\n",
    "        tkn.append(word)\n",
    "#storing all tokens from each pdf into a single variable         \n",
    "\n",
    "tkn_set = list(set(tkn))\n",
    "#storing unique tokens in tkn_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = len(tkn_set)\n",
    "val = [0]*l\n",
    "#creating list of values to be zipped into a dictionary\n",
    "\n",
    "title_dict = dict(zip(tkn_set,val))\n",
    "#creating a dictionary for title tokens\n",
    "\n",
    "for word in tkn:\n",
    "    if word in tkn_set:\n",
    "        title_dict[word] +=1\n",
    "#generating count for each word in the dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## f. Creating a data frame with top 10 most frequently used words in Titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_records([title_dict])\n",
    "df = pd.melt(df)\n",
    "#creating data frame for title tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>variable</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>learning</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>models</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>analysis</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>model</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>inference</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>networks</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>optimization</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>neural</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>bayesian</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>clustering</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       variable  value\n",
       "0      learning     50\n",
       "1        models     16\n",
       "2      analysis     15\n",
       "3         model     14\n",
       "4     inference     13\n",
       "5      networks     12\n",
       "6  optimization     10\n",
       "7        neural      9\n",
       "8      bayesian      8\n",
       "9    clustering      7"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.sort_values(['value','variable'], ascending = [False,True])\n",
    "#sorting words in dataframe in decreasing order of frequency and alphabetically \n",
    "\n",
    "df = df.drop_duplicates(subset=['value'], keep='first').reset_index().drop(columns=['index'])\n",
    "#removing duplicates for each frequences \n",
    "\n",
    "df = df.reset_index().drop(df[df.index>9].index).drop(columns=['index'])\n",
    "#getting top ten most frequently used tokens in titles\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Extracting Authors\n",
    "\n",
    "### a. Extracting Authors from each PDF file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracting the Authors from each pdf \n",
    "\n",
    "authors = [''] * 200\n",
    "#initialising list to store authors from each pdf file\n",
    "\n",
    "for i in range(1,201):\n",
    "    i = str(i)\n",
    "    fileName = \"./PDF/\" + i + \".pdf\"\n",
    "    #reading each pdf in the PDF folder\n",
    "    \n",
    "    fileContents = convert_pdf_to_txt(fileName)\n",
    "    #coverting pdf to file\n",
    "    \n",
    "    start = re.search('Authored by:', fileContents).start()\n",
    "    end = re.search('\\nAbstract', fileContents).end()\n",
    "    author = fileContents[start+14:end-10]\n",
    "    #finding start and end index of authors in each pdf and then extracting the authors\n",
    "    \n",
    "    i = int(i)\n",
    "    authors[i-1] = author\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b. Tokenizing Individual Authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors1 = [''] * 200\n",
    "#initialising list to store indivual author names\n",
    "\n",
    "for i in range(len(authors)):\n",
    "    authors1[i] = authors[i].split('\\n')\n",
    "    authors1[i] = list(filter(None, authors1[i]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c. Creating a Word Count on Unique Author Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_authors = []\n",
    "#intialising list to store unique author names\n",
    "\n",
    "for i in range(len(authors1)):\n",
    "    for word1 in authors1[i]:\n",
    "        all_authors.append(word1)\n",
    "        \n",
    "#creating a single list containg author names from all pdfs\n",
    "\n",
    "author_set = list(set(all_authors))\n",
    "#creating a list of unique authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = len(author_set)\n",
    "val = [0]*l\n",
    "#initialising list of values to be zipped with unique author names\n",
    "\n",
    "author_dict = dict(zip(author_set,val))\n",
    "#creating an author dictionary\n",
    "\n",
    "for word in all_authors:\n",
    "    if word in author_set:\n",
    "        author_dict[word] +=1\n",
    "#generating a count for each author name from all pdfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## f. Creating a data frame with top 10 most frequent authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>variable</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Aapo Hyv?rinen</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Aaron Dennis</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Aaron Roth</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Abbas Bazzi</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Abhinav Gupta</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         variable  value\n",
       "0  Aapo Hyv?rinen      1\n",
       "1    Aaron Dennis      1\n",
       "2      Aaron Roth      1\n",
       "3     Abbas Bazzi      1\n",
       "4   Abhinav Gupta      1"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "authors_df = pd.DataFrame.from_records([author_dict])\n",
    "authors_df = pd.melt(authors_df)\n",
    "#creating data frame for authors\n",
    "\n",
    "authors_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>variable</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Christopher R?</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Zoubin Ghahramani</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Inderjit S. Dhillon</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Joshua B. Tenenbaum</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Lawrence Carin</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Masashi Sugiyama</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Michael W. Mahoney</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Ryan P. Adams</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Shinichi Nakajima</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Stefano Ermon</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              variable  value\n",
       "0       Christopher R?      4\n",
       "1    Zoubin Ghahramani      4\n",
       "2  Inderjit S. Dhillon      3\n",
       "3  Joshua B. Tenenbaum      3\n",
       "4       Lawrence Carin      3\n",
       "5     Masashi Sugiyama      3\n",
       "6   Michael W. Mahoney      3\n",
       "7        Ryan P. Adams      3\n",
       "8    Shinichi Nakajima      3\n",
       "9        Stefano Ermon      3"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "authors_df = authors_df.sort_values(['value','variable'], ascending = [False,True]).reset_index().drop(columns=['index'])\n",
    "#sorting words in dataframe in decreasing order of frequency and alphabetically \n",
    "\n",
    "authors_df = authors_df.reset_index().drop(authors_df[authors_df.index>9].index).drop(columns=['index'])\n",
    "#getting top ten most frequently used authors\n",
    "\n",
    "authors_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Extracting Abstracts\n",
    "\n",
    "### a. Extracting Abstracts from each PDF file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracting the Abstracts from each pdf \n",
    "\n",
    "abstracts = [''] * 200\n",
    "#initialising list to store individual abstracts\n",
    "\n",
    "for i in range(1,201):\n",
    "    i = str(i)\n",
    "    fileName = \"./PDF/\" + i + \".pdf\"\n",
    "    #reading each pdf in the PDF folder\n",
    "    \n",
    "    fileContents = convert_pdf_to_txt(fileName)\n",
    "    #coverting pdf to file\n",
    "    \n",
    "    fileContents = re.sub('\\n', ' ', fileContents)\n",
    "    start = re.search('Abstract', fileContents).start()\n",
    "    end = re.search('1 Paper Body', fileContents).end()\n",
    "    abstract = fileContents[start+8:end-12]\n",
    "    #finding start and end index of abstracts in each pdf and then extracting the abstracts\n",
    "    \n",
    "    i = int(i)\n",
    "    abstracts[i-1] = abstract.strip()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b. Performing Sentence Segmentation for Abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract_tk1 = abstracts\n",
    "#creating duplicate list\n",
    "\n",
    "for i in range(len(abstract_tk1)):\n",
    "    abstract_tk1[i] = sent_detector.tokenize(abstract_tk1[i].strip())\n",
    "#Sentence segmentation on individual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c. Normalising first letter of each sentence to lower-case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(abstract_tk1)):\n",
    "    for j in range(len(abstract_tk1[i])):\n",
    "        abstract_tk1[i][j] = abstract_tk1[i][j][0].lower() + abstract_tk1[i][j][1:]\n",
    "#normalising the first word of each line to lower case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## d. Performing Word Tokenisation on Abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract_tokens = [[] for i in range(len(abstract_tk1))]\n",
    "#initialising a list to store abstract tokens\n",
    "\n",
    "for i in range(len(titles)):\n",
    "    abstract_tokens[i] = str(abstract_tk1[i])\n",
    "    abstract_tokens[i] = re.sub('-\\s','',abstract_tokens[i])\n",
    "    abstract_tokens[i] = re.findall(r\"[A-Za-z]\\w+(?:[-'?]\\w+)?\", abstract_tokens[i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## e. Removing context independent Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract_tokens2 = [[]for i in range(len(abstract_tokens))]\n",
    "#initialising a list to store filtered tokens\n",
    "\n",
    "for i in range(len(abstract_tokens)):\n",
    "    for words in abstract_tokens[i]:\n",
    "        if words.lower() not in stopWords:\n",
    "            abstract_tokens2[i].append(words)\n",
    "#filtering out stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## f. Creating a Word Count on unique words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = []\n",
    "#initialising a list to store unique words\n",
    "\n",
    "for i in range(len(abstract_tokens2)):\n",
    "    for word in abstract_tokens2[i]:\n",
    "        all_words.append(word)\n",
    "#appending all words from each pdf into a single variable\n",
    "\n",
    "word_set = list(set(all_words))\n",
    "#getting unique set of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = len(word_set)\n",
    "val = [0]*l\n",
    "#initialising list to be zippef with word set\n",
    "\n",
    "word_dict = dict(zip(word_set,val))\n",
    "#creating a dictionary for unique words\n",
    "\n",
    "for word in all_words:\n",
    "    if word in word_set:\n",
    "        word_dict[word] +=1\n",
    "#generating word count for each word in the dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## g. Creating a data frame with top 10 most frequently used words in Abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>variable</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ADMM</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ADMiRA</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AMP</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Abstract</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Action</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   variable  value\n",
       "0      ADMM      3\n",
       "1    ADMiRA      1\n",
       "2       AMP      1\n",
       "3  Abstract      1\n",
       "4    Action      1"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_df = pd.DataFrame.from_records([word_dict])\n",
    "word_df = pd.melt(word_df)\n",
    "#creating a dataframe for all words in abstracts\n",
    "\n",
    "word_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>variable</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>learning</td>\n",
       "      <td>182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>model</td>\n",
       "      <td>176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>algorithm</td>\n",
       "      <td>134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>data</td>\n",
       "      <td>131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>problem</td>\n",
       "      <td>119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>algorithms</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>show</td>\n",
       "      <td>109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>models</td>\n",
       "      <td>94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>methods</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>approach</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     variable  value\n",
       "0    learning    182\n",
       "1       model    176\n",
       "2   algorithm    134\n",
       "3        data    131\n",
       "4     problem    119\n",
       "5  algorithms    111\n",
       "6        show    109\n",
       "7      models     94\n",
       "8     methods     92\n",
       "9    approach     81"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_df2 = word_df.sort_values(['value','variable'], ascending = [False,True])\n",
    "#sorting words in dataframe in decreasing order of frequency and alphabetically \n",
    "\n",
    "word_df2 = word_df2.drop_duplicates(subset=['value'], keep='first').reset_index().drop(columns=['index'])\n",
    "#removing duplicates for each frequences\n",
    "\n",
    "word_df2 = word_df2.reset_index().drop(word_df2[word_df2.index>9].index).drop(columns=['index'])\n",
    "#getting top ten most frequently used words in abstracts\n",
    "\n",
    "word_df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Generating a CSV file containing top 10 most frequently used terms in titles, frequent authors, and frequently used terms in abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>top10_terms_in_titles</th>\n",
       "      <th>top10_authors</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top10_terms_in_abstracts</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ADMM</th>\n",
       "      <td>learning</td>\n",
       "      <td>Christopher R?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ADMiRA</th>\n",
       "      <td>models</td>\n",
       "      <td>Zoubin Ghahramani</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AMP</th>\n",
       "      <td>analysis</td>\n",
       "      <td>Inderjit S. Dhillon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Abstract</th>\n",
       "      <td>model</td>\n",
       "      <td>Joshua B. Tenenbaum</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Action</th>\n",
       "      <td>inference</td>\n",
       "      <td>Lawrence Carin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Adaptive</th>\n",
       "      <td>networks</td>\n",
       "      <td>Masashi Sugiyama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Al-Sc</th>\n",
       "      <td>optimization</td>\n",
       "      <td>Michael W. Mahoney</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Alan</th>\n",
       "      <td>neural</td>\n",
       "      <td>Ryan P. Adams</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Allocation</th>\n",
       "      <td>bayesian</td>\n",
       "      <td>Shinichi Nakajima</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Analysis</th>\n",
       "      <td>clustering</td>\n",
       "      <td>Stefano Ermon</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         top10_terms_in_titles        top10_authors\n",
       "top10_terms_in_abstracts                                           \n",
       "ADMM                                  learning       Christopher R?\n",
       "ADMiRA                                  models    Zoubin Ghahramani\n",
       "AMP                                   analysis  Inderjit S. Dhillon\n",
       "Abstract                                 model  Joshua B. Tenenbaum\n",
       "Action                               inference       Lawrence Carin\n",
       "Adaptive                              networks     Masashi Sugiyama\n",
       "Al-Sc                             optimization   Michael W. Mahoney\n",
       "Alan                                    neural        Ryan P. Adams\n",
       "Allocation                            bayesian    Shinichi Nakajima\n",
       "Analysis                            clustering        Stefano Ermon"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = df.reset_index().drop(columns=['value'])\n",
    "authors_df2 = authors_df.reset_index().drop(columns=['value'])\n",
    "word_df2 = word_df.reset_index().drop(columns=['value'])\n",
    "#dropping the count from each of the dataframes\n",
    "\n",
    "stats_df = pd.merge(word_df2, df2, on='index')\n",
    "stats_df2 = pd.merge(stats_df, authors_df2, on='index').drop(columns=['index']).rename(columns ={'variable_x':'top10_terms_in_abstracts','variable_y':'top10_terms_in_titles','variable':'top10_authors'}).set_index('top10_terms_in_abstracts')\n",
    "#merging stats into a single dataframe\n",
    "\n",
    "stats_df2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_df2.to_csv('./Group058_stats.csv')\n",
    "#generating csv file"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
